"""
Generate response to the question posed using RAG with Retrieval components created from retrievals.py, followed by Langchain pipeline
to answer the question
"""

#Import statements
import os #os file paths
import logging  # Set logging for the queries
from pathlib import Path #Manipulate file path names

#Import statements Langchain
from langchain.chains.retrieval import create_retrieval_chain #RAG Chain
from langchain.chains.combine_documents import create_stuff_documents_chain #Retrieval Part of Chain
from langchain_core.prompts import ChatPromptTemplate #Chat Template for QA
from langchain_ollama.llms import OllamaLLM #LLM

#Import other modules in src
import retrievals #Set up retrieval for RAG
import custom_prompts #Custom prompts

#Global Constants
LLM_MODEL = "llama3.2" #LLM model

def main():
    #Create the embeddings
    hf_embeddings = retrievals.create_embedding_model()
    docs = retrievals.perform_ocr()
    #Create the retrievers for RAG
    parent_retriever = retrievals.create_parent_retriever(hf_embeddings,docs)
    retriever_from_llm = retrievals.create_multiqueries_retriever(parent_retriever)

    #Setup Logging to check the queries created by multi queries retriever
    setup_logging()

   
    #Create the LLM Model
    model = OllamaLLM(model=LLM_MODEL, temperature=0.2)

    #Create prompt for the rag chain
    prompt = create_chat()

    # Retrieve the documents to pass to the llm model in the create_retrieval chain
    question_answer_chain = create_stuff_documents_chain(model, prompt)
    rag_chain = create_retrieval_chain(retriever_from_llm, question_answer_chain)

    #QA
    response = rag_chain.invoke({"input": "what are the differences between the Swiss and UK acts"})
  
    #format the output with source
    generated_output = format_output_source(response)
    print(generated_output)
        
    pass

def setup_logging():
    """
        Set the logging status for checking multi queries generated during multi query retrieval
    """   
    logging.basicConfig()
    logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)

def create_chat():
    """Create chat to set the custom system prompt followed by creation of the question to allow for response to the question

    Returns:
        prompt: Messages with a system prompt and a question to allow for response to this question "chat prompt"
    """    

    #Create the chat to allow for response to the question under {"human": question}
    prompt = ChatPromptTemplate.from_messages(
            [
                ("system", custom_prompts.CHAT_SYSTEM_PROMPT),
                ("human", "{input}"),
            ]
        )
    return prompt

def format_output_source(response):
    """Return the response by the LLM formatted with sources

    Args:
        response: Response generated by LLM in Dictionary format with context keys and answer keys

    Returns:
        generated_output: Response formatted with sources at the end in the form of Document: {set of page numbers}
    """    

    #Create dict in the form of Dict: set(page numbers)
    source_dict = {}
    #Iterate through every retrieverd chunks
    for doc in response["context"]:
        full_path = doc.metadata["source"]
        #Retrieve only the stem (last / of the path)
        stem_path = Path(full_path).stem
        #Retrieve the page number in the document
        page = doc.metadata["page"]
        #Create if the document is not present
        if stem_path not in source_dict:
            source_dict[stem_path] = set()
        #Add the page number
        source_dict[stem_path].add(page)
    
    generated_output = str(response["answer"])
    generated_output += f"\n\nSources in Document and Page Numbers format:"
    #Add the sources in Doc : (page numbers) format to the end of the response
    for source, page_numbers in source_dict.items():
        #page_numbers = str(list)
        generated_output += f"\n{source}: {page_numbers} "
    
    return generated_output



if __name__ == "__main__":
    main()
    


